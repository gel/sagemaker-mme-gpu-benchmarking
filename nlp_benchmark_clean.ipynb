{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5a31fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark RoBERTa base model using Amazon SageMaker Multi-model endpoints (MME) with GPU support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU works using **NVIDIA Triton Inference Server**.\n",
    "\n",
    "NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and **provides high inference performance**. Triton supports all major training and inference frameworks.\n",
    "It offers **dynamic batching, concurrent execution, post-training quantization, optimal model configuration** to achieve high performance inference.\n",
    "\n",
    "In this notebook, we are going to run benchmark testing for the most popular NLP models using MME on GPU. **We will evaluate model performance such as the inference latency, throughput, and optimum model count per instance.**\n",
    "\n",
    "This notebook is tested on `PyTorch 1.12 Python 3.8 CPU Optimized` kernel on SageMaker Studio. An instance with at least 8 vCPU cores such as an `ml.c5.2xlarge` is recommended to run the load test. A smaller instance may be utilized by reducing the scale of the load test. The configuration provided here can simulate up to 200 concurrent workers."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b237a7c",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1552a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install timm -Uqq\n",
    "%pip install transformers -Uqq\n",
    "%pip install locust -Uqq\n",
    "%pip install boto3 -Uqq\n",
    "%pip install sagemaker -Uqq\n",
    "%pip install matplotlib -Uqq\n",
    "%pip install Jinja2 -Uqq\n",
    "%pip install ipywidgets -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e659fb-6fe6-4cbd-8f45-890e531cce46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec1001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b4725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "from utils import model_utils\n",
    "\n",
    "region = boto3.Session().region_name\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "account = sess.account_id()\n",
    "bucket = sess.default_bucket()\n",
    "\n",
    "model_name = \"roberta-base\" \n",
    "prefix = 'mme-roberta-base-benchmark'\n",
    "use_case = \"nlp\"\n",
    "max_seq_len = 128 # sequence length\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d4db3",
   "metadata": {},
   "source": [
    "## Generate Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fed9ec",
   "metadata": {},
   "source": [
    "We are going to use the following SageMaker Processing script to generate our pretrained model.\n",
    "Helper functions have been created for each of these steps and are imported from the `utils.model_utils` local module\n",
    "\n",
    "#### Returns a model and tokenizer from HuggingFace Hub.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0bad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tokenizer, model = model_utils.get_model_from_hf_hub(model_name)\n",
    "model.eval()\n",
    "print(f\"loaded model {model_name} with {model_utils.count_parameters(model)} parameters\")\n",
    "example_input = tokenizer(\"This is a sample\", padding=\"max_length\", max_length=max_seq_len, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea9a8cb9-b162-473a-a2ef-3bd2d47387d3",
   "metadata": {},
   "source": [
    "#### jit script the model and save the torchscript file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6fba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_model_path = Path(f\"triton-serve-pt/{model_name}/1\")\n",
    "pytorch_model_path.mkdir(parents=True, exist_ok=True)\n",
    "pt_model_path = model_utils.export_pt_jit(model, list(example_input.values()), pytorch_model_path) #export jit compiled model to specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dae48",
   "metadata": {},
   "source": [
    "#### Create and package a model artifact \n",
    "\n",
    "The package contains torchscript file and a model configuration (config.pbtxt) for Triton serving\n",
    "It is possible to configure the the input and output for triton according to your model.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eff00a-c60e-4165-9b63-91a2cda829c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_inputs = [\n",
    "    {\"name\": input_name, \"data_type\": \"TYPE_INT32\", \"dims\": f\"[{max_seq_len}]\"} \n",
    "        for input_name in example_input\n",
    "]\n",
    "triton_outputs = [\n",
    "    {\n",
    "        \"name\": \"last_hidden_state\",\n",
    "        \"data_type\": \"TYPE_FP32\",\n",
    "        \"dims\": f\"[{max_seq_len}, {model.config.hidden_size}]\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8066a-b2a2-451c-914f-d7740d789623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_config_path = model_utils.generate_triton_config(platform=\"pt\", triton_inputs=triton_inputs, triton_outputs=triton_outputs, save_path=pytorch_model_path)\n",
    "triton_config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9f4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_artifact_path = model_utils.package_triton_model(model_name, pt_model_path, triton_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85a69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path = f\"s3://{bucket}/{prefix}/{model_name}/\"\n",
    "initial_model_path = sess.upload_data(model_artifact_path.as_posix(), bucket=bucket, key_prefix=f\"{prefix}{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70241",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "initial_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65b114",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c13fc9",
   "metadata": {},
   "source": [
    "#### We verify that no models located in the Multi Model Endpoint S3 Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992f2a19-2c50-4d2a-abc2-e58c50b76041",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 rm --recursive {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2d4df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859b150",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b99513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.endpoint_utils import create_endpoint, delete_endpoint, get_instance_utilization, run_load_test\n",
    "from utils.model_utils import get_triton_image_uri\n",
    "\n",
    "mme_triton_image_uri = get_triton_image_uri(region)\n",
    "print(mme_triton_image_uri)\n",
    "instance_type = 'ml.g4dn.xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248712a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": mme_path,\n",
    "    \"Mode\": \"MultiModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875fa1e",
   "metadata": {},
   "source": [
    "We'll deploy and endpoint is deployed using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3dbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, container, instance_type, \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccd9b7",
   "metadata": {},
   "source": [
    "Next we'll upload a python model that we can use to query the instance utilization in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291029f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar czvf metrics.tar.gz server_metrics/\n",
    "!aws s3 cp metrics.tar.gz {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c5e3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb93dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6999a89",
   "metadata": {},
   "source": [
    "## Load PyTorch Models into Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa7826",
   "metadata": {},
   "source": [
    "In this section we will determine the maximum number of model copies that the endpoint can load into memory within a specified threshold\n",
    "- When a model is invoked for the first time, SageMaker will load it into the GPU Memory\n",
    "- In this section we will invoke the model with a sample endpoint which result in it being loaded into memory\n",
    "- We'll then make copies of the model on S3 and invoke each copy until we reach the specified GPU Memory threshold which we set at 90% of Available memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c24fd-5c57-4c62-853d-fc7f28a3c1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": list(data.size()), \"datatype\": \"INT32\", \"data\": data.tolist()} for name, data in example_input.items()]\n",
    "}\n",
    "payload['inputs'][0]['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa65813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b341933-d503-4262-816a-7f7257f32df4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823ebd12-db0a-453e-8b30-d26ab5d1532f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4741b",
   "metadata": {},
   "source": [
    "## Benchmark Pytorch Model using Locust\n",
    "\n",
    "`locust_benchmark_sm.py` is provided in the 'locust' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541e166",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The load test is run with up to 200 simulated workers. This may not be suitable for larger models with long response times. You can modify the <code>StagesShape</code> Class in the <code>locust/locust_benchmark_sm.py</code> file to adjust the traffic pattern and the number of concurrent workers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d72f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3de682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*pt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, use_case, model_name, models_loaded, output_path, print_stdout=True, n_procs=6, sample_payload=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200954b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import some utilities to analyze the results of the load test\n",
    "from utils.viz_utils import get_summary_results, generate_summary_plots, generate_metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa859a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e34eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb4281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad61b8a",
   "metadata": {},
   "source": [
    "## Benchmark models invocation times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49ba8c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def invoke_models_sequentially(models_loaded, full_models_loop_counter):\n",
    "    total_time = 0\n",
    "    for x in range (full_models_loop_counter):\n",
    "        total_loop_time = 0\n",
    "        for counter in range(models_loaded):   \n",
    "                st = time.time()\n",
    "                target_model= f\"{model_name}-v{counter}.tar.gz\"\n",
    "                print(f\"invoking model {target_model}\")\n",
    "                response = runtime_sm_client.invoke_endpoint(\n",
    "                            EndpointName=endpoint_name,\n",
    "                            ContentType=\"application/octet-stream\",\n",
    "                            Body=json.dumps(payload),\n",
    "                            TargetModel=target_model, \n",
    "                        )\n",
    "                response = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "                output = response[\"outputs\"][0][\"data\"]\n",
    "                # print(len(output))\n",
    "                et = time.time()\n",
    "                elapsed_time = et - st\n",
    "                total_loop_time += elapsed_time\n",
    "                total_time += elapsed_time\n",
    "                print('Execution time:', elapsed_time, 'seconds')\n",
    "        print(f'\\n***Invoked models: {models_loaded}, Total loop time: {total_loop_time}, Average loop time: {total_loop_time/models_loaded}\\n')\n",
    "\n",
    "    print('\\n------------------------\\n')\n",
    "    print(f'Amount of models invoked: {full_models_loop_counter*models_loaded}')\n",
    "    print(f'Total time: {total_time}')\n",
    "    print(f'Average time: {total_time/(full_models_loop_counter*models_loaded)}')\n",
    "    \n",
    "invoke_models_sequentially(models_loaded, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50973138-36ac-45db-aac6-f920d2f0abb1",
   "metadata": {},
   "source": [
    "## Benchmark 2 X models invocation times\n",
    "\n",
    "We will add additional models to reach to total of twice the amount of models loaded. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb966a4-bed8-402e-8f0f-cfca3b9d2b81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "target_models_to_load = models_loaded*2\n",
    "target_models_to_load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18522ca-05de-4a1c-b210-3f9eac861067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while models_loaded < target_models_to_load:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    models_loaded+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dfcac-8588-4ba4-85a9-fcee193ff91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da000871-c555-4685-8026-07efb9c49662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "invoke_models_sequentially(models_loaded, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f71c8-089e-4da1-b3ce-255fd7981024",
   "metadata": {},
   "source": [
    "## Clean Up PyTorch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85195772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {mme_path}"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "bd01d8a33ae048de921784525e60c8784d22ac368cf7370d33c1ec56f2410197"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
