{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c5a31fc",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Benchmark RoBERTa base model using Amazon SageMaker Multi-model endpoints (MME) with GPU support\n",
    "\n",
    "Amazon SageMaker multi-model endpoints with GPU works using NVIDIA Triton Inference Server. NVIDIA Triton Inference Server is open-source inference serving software that simplifies the inference serving process and provides high inference performance. Triton supports all major training and inference frameworks, such as TensorFlow, NVIDIA TensorRT, PyTorch, MXNet, Python, ONNX, XGBoost, scikit-learn, RandomForest, OpenVINO, custom C++, and more. It offers dynamic batching, concurrent execution, post-training quantization, optimal model configuration to achieve high performance inference.\n",
    "\n",
    "In this notebook, we are going to run benchmark testing for the most popluar NLP models using MME on GPU. We will evaluate model performance such as the inference latency, throughput, and optimum model count per instance. We will also compile these models using NVIDA TensorRT to compare performance against TorchScript models.\n",
    "\n",
    "This notebook is tested on `PyTorch 1.12 Python 3.8 CPU Optimized` kernel on SageMaker Studio. An instance with at least 8 vCPU cores such as an `ml.c5.2xlarge` is recommended to run the load test. A smaller instance may be utilized by reducing the scale of the load test. The configuration provide here simulates up to 200 concurrent workers      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b237a7c",
   "metadata": {},
   "source": [
    "## Set up the environment\n",
    "\n",
    "Installs the dependencies required to package the model and run inferences using Triton server.\n",
    "\n",
    "Also define the IAM role that will give SageMaker access to the model artifacts and the NVIDIA Triton ECR image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1552a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%pip install timm -Uqq\n",
    "%pip install transformers -Uqq\n",
    "%pip install locust -Uqq\n",
    "%pip install boto3 -Uqq\n",
    "%pip install sagemaker -Uqq\n",
    "%pip install matplotlib -Uqq\n",
    "%pip install Jinja2 -Uqq\n",
    "%pip install ipywidgets -Uqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e659fb-6fe6-4cbd-8f45-890e531cce46",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import IPython\n",
    "\n",
    "IPython.Application.instance().kernel.do_shutdown(True)  # has to restart kernel so changes are used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bec1001",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%env TOKENIZERS_PARALLELISM=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc8b4725",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "from pathlib import Path\n",
    "import time\n",
    "import datetime as dt\n",
    "import warnings\n",
    "\n",
    "from utils import model_utils\n",
    "\n",
    "role = get_execution_role()\n",
    "sess = sagemaker.Session()\n",
    "\n",
    "account = sess.account_id()\n",
    "bucket = sess.default_bucket() # or use your own custom bucket name\n",
    "prefix = 'mme-roberta-base-benchmark'\n",
    "\n",
    "use_case = \"nlp\"\n",
    "\n",
    "sm_client = boto3.client(service_name=\"sagemaker\")\n",
    "runtime_sm_client = boto3.client(\"sagemaker-runtime\")\n",
    "\n",
    "tested_models = [\"roberta-base\"]\n",
    "\n",
    "model_name = \"roberta-base\" #change the model name to benchmark different NLP models\n",
    "\n",
    "max_seq_len = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5503b2",
   "metadata": {},
   "source": [
    "Account Id Mapping for triton inference containers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1041ca81",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "account_id_map = {\n",
    "    'us-east-1': '785573368785',\n",
    "    'us-east-2': '007439368137',\n",
    "    'us-west-1': '710691900526',\n",
    "    'us-west-2': '301217895009',\n",
    "    'eu-west-1': '802834080501',\n",
    "    'eu-west-2': '205493899709',\n",
    "    'eu-west-3': '254080097072',\n",
    "    'eu-north-1': '601324751636',\n",
    "    'eu-south-1': '966458181534',\n",
    "    'eu-central-1': '746233611703',\n",
    "    'ap-east-1': '110948597952',\n",
    "    'ap-south-1': '763008648453',\n",
    "    'ap-northeast-1': '941853720454',\n",
    "    'ap-northeast-2': '151534178276',\n",
    "    'ap-southeast-1': '324986816169',\n",
    "    'ap-southeast-2': '355873309152',\n",
    "    'cn-northwest-1': '474822919863',\n",
    "    'cn-north-1': '472730292857',\n",
    "    'sa-east-1': '756306329178',\n",
    "    'ca-central-1': '464438896020',\n",
    "    'me-south-1': '836785723513',\n",
    "    'af-south-1': '774647643957'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5abb439f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "if region not in account_id_map.keys():\n",
    "    raise(\"UNSUPPORTED REGION\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "226d4db3",
   "metadata": {},
   "source": [
    "## Generate Pretrained Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fed9ec",
   "metadata": {},
   "source": [
    "We are going to use the following SageMaker Processing script to generate our pretrained model. This script does the following:\n",
    "\n",
    "1. Generate a model using the Pytorch Hub\n",
    "\n",
    "2. jit script the model and save the torchscript file\n",
    "\n",
    "3. Create a model artifact which is comprised of the torchscript file and a model configuration (config.pbtxt) for Triton serving\n",
    "\n",
    "Helper functions have been created for each of these steps and are imported from the `utils.model_utils` local module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87f0bad5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model_name in tested_models:\n",
    "    tokenizer, model = model_utils.get_model_from_hf_hub(model_name)\n",
    "else:\n",
    "    warnings.warn(f\"{model_name} has not been tested and may not work\")\n",
    "    tokenizer, model = model_utils.get_model_from_hf_hub(model_name)\n",
    "model.eval()\n",
    "\n",
    "print(f\"loaded model {model_name} with {model_utils.count_parameters(model)} parameters\")\n",
    "\n",
    "example_input = tokenizer(\"This is a sample\", padding=\"max_length\", max_length=max_seq_len, return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2976213f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Packaging Pytorch model for Triton sever on SageMaker\n",
    "\n",
    "**Note**: SageMaker expects the model tarball file to have a top level directory with the same name as the model defined in the `config.pbtxt`.\n",
    "\n",
    "```\n",
    "model_name\n",
    "├── 1\n",
    "│   └── model.pt\n",
    "└── config.pbtxt\n",
    "```\n",
    "\n",
    "We will be tracing an existing RoBERTa base model for the purpose of converting PyTorch modules to TorchScript - PyTorch high-performance deployment runtime. \n",
    "\n",
    "[This tutorial](https://pytorch.org/tutorials/beginner/Intro_to_TorchScript_tutorial.html) is an introduction to TorchScript, an intermediate representation of a PyTorch model (subclass of nn.Module) that can then be run in a high-performance environment such as C++."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b6fba1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pytorch_model_path = Path(f\"triton-serve-pt/{model_name}/1\")\n",
    "pytorch_model_path.mkdir(parents=True, exist_ok=True)\n",
    "pt_model_path = model_utils.export_pt_jit(model, list(example_input.values()), pytorch_model_path) #export jit compiled model to specified directory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e8dae48",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "Based on the architecture of the model we will generate a Triton configuration (config.pbtxt) file. This approach should work for most models but you may need to make adjustments to the generated config. Additionally a base model is assumed that will return the output from the last hidden state. If using a different output head such as a sequence classification, adjust the triton_outputs variable below.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1eff00a-c60e-4165-9b63-91a2cda829c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#get input names \n",
    "triton_inputs = [\n",
    "    {\"name\": input_name, \"data_type\": \"TYPE_INT32\", \"dims\": f\"[{max_seq_len}]\"}\n",
    "    for input_name in example_input\n",
    "]\n",
    "triton_outputs = [\n",
    "    {\n",
    "        \"name\": \"last_hidden_state\",\n",
    "        \"data_type\": \"TYPE_FP32\",\n",
    "        \"dims\": f\"[{max_seq_len}, {model.config.hidden_size}]\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec8066a-b2a2-451c-914f-d7740d789623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "triton_config_path = model_utils.generate_triton_config(platform=\"pt\", triton_inputs=triton_inputs,  triton_outputs=triton_outputs, save_path=pytorch_model_path)\n",
    "triton_config_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e9f4f0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We'll package a model config template along with the compiled model into a model.tar.gz artifact. \n",
    "# The config templates assume batch size of 32 and sequence length of 128\n",
    "# You may need to adjust the template if not using one of the tested models\n",
    "model_atifact_path = model_utils.package_triton_model(model_name, pt_model_path, triton_config_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf85a69c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mme_path = f\"s3://{bucket}/{prefix}/{model_name}/\"\n",
    "initial_model_path = sess.upload_data(model_atifact_path.as_posix(), bucket=bucket, key_prefix=f\"{prefix}{model_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fe70241",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_model_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c65b114",
   "metadata": {},
   "outputs": [],
   "source": [
    "mme_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c13fc9",
   "metadata": {},
   "source": [
    "No model will be located in the Multi Model Endpoint path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32f2d4df",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a859b150",
   "metadata": {},
   "source": [
    "## Create a SageMaker Multi-Model Endpoint for PyTorch Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7b99513",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from utils.endpoint_utils import create_endpoint, delete_endpoint, get_instance_utilization, run_load_test\n",
    "\n",
    "base = \"amazonaws.com.cn\" if region.startswith(\"cn-\") else \"amazonaws.com\"\n",
    "mme_triton_image_uri = f\"{account_id_map[region]}.dkr.ecr.{region}.{base}\" + \\\n",
    "            \"/sagemaker-tritonserver:22.10-py3\"\n",
    "print(mme_triton_image_uri)\n",
    "instance_type = 'ml.g4dn.xlarge'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9248712a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "container = {\n",
    "    \"Image\": mme_triton_image_uri,\n",
    "    \"ModelDataUrl\": mme_path,\n",
    "    \"Mode\": \"MultiModel\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4875fa1e",
   "metadata": {},
   "source": [
    "We'll deploy and endpoint is deployed using a helper function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb3dbc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sm_model_name, endpoint_config_name, endpoint_name = create_endpoint(sm_client, model_name, role, container, instance_type, \"pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ccd9b7",
   "metadata": {},
   "source": [
    "Next we'll upload a python model that we can use to query the instance utilization in real time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291029f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!tar czvf metrics.tar.gz server_metrics/\n",
    "!aws s3 cp metrics.tar.gz {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f8c5e3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eb93dab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "get_instance_utilization(runtime_sm_client, endpoint_name) #invoke once to load the python model in memory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6999a89",
   "metadata": {},
   "source": [
    "## Load PyTorch Models into Endpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cafa7826",
   "metadata": {},
   "source": [
    "In this section we will determine the maximum number of model copies that the endpoint can load into memory within a specified threshold\n",
    "- When a model is invoked for the first time, SageMaker will load it into the GPU Memory\n",
    "- In this section we will invoke the model with a sample endpoint which result in it being loaded into memory\n",
    "- We'll then make copies of the model on S3 and invoke each copy until we reach the specified GPU Memory threshold which we set at 90% of Available memory "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e23c24fd-5c57-4c62-853d-fc7f28a3c1b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "payload = {\n",
    "    \"inputs\":\n",
    "        [{\"name\": name, \"shape\": list(data.size()), \"datatype\": \"INT32\", \"data\": data.tolist()} for name, data in example_input.items()]\n",
    "}\n",
    "payload['inputs'][0]['shape']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa65813",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "models_loaded = 0\n",
    "memory_utilization_threshold = 0.9\n",
    "memory_utilization_history = []\n",
    "while True:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    \n",
    "    # make a inference request to load model into memory\n",
    "    response = runtime_sm_client.invoke_endpoint(\n",
    "            EndpointName=endpoint_name,\n",
    "            ContentType=\"application/octet-stream\",\n",
    "            Body=json.dumps(payload),\n",
    "            TargetModel=f\"{model_name}-v{models_loaded}.tar.gz\", \n",
    "        )\n",
    "    \n",
    "    models_loaded+=1\n",
    "    \n",
    "    #get instance metrics\n",
    "    instance_metrics = get_instance_utilization(runtime_sm_client, endpoint_name)\n",
    "    model_avg_mem_consumption = instance_metrics[\"gpu_used_memory\"] / models_loaded\n",
    "    \n",
    "    # get an estimate of the gpu memory util once next model is loaded\n",
    "    next_gpu_mem_util = (instance_metrics[\"gpu_used_memory\"] + model_avg_mem_consumption) / instance_metrics[\"gpu_total_memory\"]\n",
    "    \n",
    "    memory_utilization = instance_metrics[\"gpu_memory_utilization\"]\n",
    "    memory_utilization_history.append(memory_utilization)\n",
    "    \n",
    "    # terminate loop if the memory consumption is exceeded once next model is loaded\n",
    "    if next_gpu_mem_util >= memory_utilization_threshold:\n",
    "        print(f\"This instance is able to load {models_loaded} models with {memory_utilization:.2%} of gpu memory consumed\")\n",
    "        break\n",
    "        \n",
    "    print(f\"loaded {models_loaded} models with memory utilzation of {memory_utilization:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a59b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce4741b",
   "metadata": {},
   "source": [
    "## Benchmark Pytorch Model using Locust\n",
    "\n",
    "`locust_benchmark_sm.py` is provided in the 'locust' folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e541e166",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-info\"> <strong> Note: </strong>\n",
    "The load test is run with up to 200 simulated workers. This may not be suitable for larger models with long response times. You can modify the <code>StagesShape</code> Class in the <code>locust/locust_benchmark_sm.py</code> file to adjust the traffic pattern and the number of concurrent workers\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d72f1f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locust_result_path = Path(\"results\") / model_name\n",
    "locust_result_path.mkdir(parents=True,exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3de682",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "output_path = (locust_result_path / f\"{instance_type}*pt*{models_loaded}\") # capture the instance type, engine, and models loaded in file name\n",
    "run_load_test(endpoint_name, use_case, model_name, models_loaded, output_path, print_stdout=True, n_procs=6, sample_payload=json.dumps(payload))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200954b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import some utilities to analyze the results of the load test\n",
    "from utils.viz_utils import get_summary_results, generate_summary_plots, generate_metrics_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8aa859a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f19e34eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "load_test_summary = get_summary_results(locust_result_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cdb4281",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "generate_summary_plots(load_test_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cad61b8a",
   "metadata": {},
   "source": [
    "## Benchmark Pytorch Model load times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b49ba8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def invoke_models_sequentially(models_loaded, full_models_loop_counter):\n",
    "    total_time = 0\n",
    "    for x in range (full_models_loop_counter):\n",
    "        total_loop_time = 0\n",
    "        for counter in range(models_loaded):   \n",
    "                st = time.time()\n",
    "                target_model= f\"{model_name}-v{counter}.tar.gz\"\n",
    "                print(f\"invoking model {target_model}\")\n",
    "                response = runtime_sm_client.invoke_endpoint(\n",
    "                            EndpointName=endpoint_name,\n",
    "                            ContentType=\"application/octet-stream\",\n",
    "                            Body=json.dumps(payload),\n",
    "                            TargetModel=target_model, \n",
    "                        )\n",
    "                response = json.loads(response[\"Body\"].read().decode(\"utf8\"))\n",
    "                output = response[\"outputs\"][0][\"data\"]\n",
    "                # print(len(output))\n",
    "                et = time.time()\n",
    "                elapsed_time = et - st\n",
    "                total_loop_time += elapsed_time\n",
    "                total_time += elapsed_time\n",
    "                print('Execution time:', elapsed_time, 'seconds')\n",
    "        print(f'\\n***Invoked models: {models_loaded}, Total loop time: {total_loop_time}, Average loop time: {total_loop_time/models_loaded}\\n')\n",
    "\n",
    "    print('\\n------------------------\\n')\n",
    "    print(f'Amount of models invoked: {full_models_loop_counter*models_loaded}')\n",
    "    print(f'Total time: {total_time}')\n",
    "    print(f'Average time: {total_time/(full_models_loop_counter*models_loaded)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70874bbe-1dfc-4f7c-b6da-94586d0dd3a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "invoke_models_sequentially(models_loaded, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50973138-36ac-45db-aac6-f920d2f0abb1",
   "metadata": {},
   "source": [
    "## Adding more models \n",
    "\n",
    "We will add additional models to reach to total of 50. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b18522ca-05de-4a1c-b210-3f9eac861067",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "while models_loaded < 50:\n",
    "    # make a copy of the model\n",
    "    !aws s3 cp {initial_model_path} {mme_path}{model_name}-v{models_loaded}.tar.gz\n",
    "    models_loaded+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38dfcac-8588-4ba4-85a9-fcee193ff91c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!aws s3 ls {mme_path}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4957779-b03b-4855-b99f-6498a311393c",
   "metadata": {},
   "source": [
    "## Benchmark Pytorch Model load times with 50 models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da000871-c555-4685-8026-07efb9c49662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "invoke_models_sequentially(models_loaded, 5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "279f71c8-089e-4da1-b3ce-255fd7981024",
   "metadata": {},
   "source": [
    "## Clean Up PyTorch Endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85195772",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "delete_endpoint(sm_client, sm_model_name, endpoint_config_name, endpoint_name)\n",
    "! aws s3 rm --recursive {mme_path}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0351464-a625-45f7-b7b9-56d75253cb50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.c5.2xlarge",
  "kernelspec": {
   "display_name": "Python 3 (PyTorch 1.10 Python 3.8 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/pytorch-1.10-cpu-py38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
